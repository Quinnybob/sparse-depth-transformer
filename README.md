# ðŸ”€ SparseDepthTransformer: Per-Token Dynamic Depth Routing for Efficient Transformers

> A novel transformer architecture that routes each token through a variable number of layers based on semantic importance â€” reducing memory usage and unnecessary compute.

---

## ðŸ§  Motivation

Modern transformer models waste compute by sending **every token** through **every layer**, regardless of how much semantic content each token actually carries.

But do common tokens like "the" or "and" really need the same deep processing as "DNA" or "restructure"?

**SparseDepthTransformer** introduces a token-wise routing mechanism that decides â€” in real-time â€” how many layers a token should pass through, based on its **semantic density**. This enables **layer skipping** on a per-token basis, which is rarely explored in transformer research.

---

## ðŸš€ Core Idea

- Score each tokenâ€™s semantic importance using a learned linear probe
- Route important tokens through all layers
- Let less important tokens **skip deeper layers**
- Use **hard skipping** (not soft blending) to save real compute

This leads to a transformer that is **both smarter and lighter**.

---

## ðŸ“Š Benchmark Results

Benchmarked on 10 runs, using sequence length = 20, batch size = 2:

| Model                    | Time (sec) | Max Memory (MB) | Avg Layers per Token |
|-------------------------|------------|------------------|-----------------------|
| **SparseDepthTransformer** | ~0.0049     | ~23.1            | ~3.6                  |
| **Baseline Transformer**   | ~0.0037     | ~27.0            | 6.0                  |

âœ… **~40% fewer layers processed per token**  
âœ… **~15% less GPU memory used**  
âš ï¸ Slight increase in latency due to token-level execution â€” batching optimization planned

---

## ðŸ“¦ Features

- âœ… Semantic scorer module
- âœ… Hard-skipping per-token per-layer
- âœ… Layer usage tracking
- âœ… Baseline transformer for comparison
- âœ… Benchmarking script for time, memory, and depth

---

## ðŸ“ How to Run

### ðŸ”§ Installation
```bash
pip install torch
```

### ðŸš€ Running the Benchmark
```bash
python main.py
```

### ðŸ§ª Run in Colab
Use this notebook: [Open Colab](https://colab.research.google.com/) *(insert your notebook link here)*

---

## ðŸ§ª Why This Project Matters

This work explores **depth sparsity**, a rarely studied axis in transformer optimization.  
While attention sparsity and Mixture-of-Experts (MoE) are popular, per-token **layer skipping** introduces a new degree of freedom â€” enabling models to **spend compute only where it's needed**.

> "Not every token deserves the same amount of thought."

This could enable:
- Lightweight models for mobile and edge AI
- Faster inference on long-context inputs
- Adaptive compute strategies in LLMs

---

## ðŸ“ˆ Future Work

- [ ] Batch tokens by routing level for GPU efficiency
- [ ] Train on real data (TinyStories, Alpaca, etc.)
- [ ] Add routing visualizations and attention heatmaps
- [ ] Integrate with HuggingFace Transformers for broader use
- [ ] Experiment with curriculum depth scheduling

---

## ðŸ§  Author

This project was created by **Quinnybob**, a high school researcher focused on efficient AI systems and transformer interpretability.

ðŸ“§ desimoneq@gmail.com
ðŸ”— [GitHub Profile](https://github.com/Quinnybob)

---

## ðŸ“¬ Contributing / Contact

Feel free to reach out if:
- You're a researcher or student interested in collaborating
- You want to integrate this into another model
- Youâ€™d like to contribute optimizations or training runs

Pull requests, ideas, and discussions are welcome!

MIT License.
