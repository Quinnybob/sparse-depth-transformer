{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjse8J89PpgY7OOjSz6zwE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Quinnybob/sparse-depth-transformer/blob/main/sparseDepthTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx_UFQTYBkm9",
        "outputId": "9054662b-7fa6-4ca0-c0e1-d1295e37d274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Sparse Depth Transformer ===\n",
            "{'time_sec': 0.004933357238769531, 'output_shape': torch.Size([2, 20, 5000]), 'max_memory_MB': 23.094784, 'avg_layers_per_token': 3.575000047683716}\n",
            "\n",
            "=== Baseline Transformer ===\n",
            "{'time_sec': 0.003687143325805664, 'output_shape': torch.Size([2, 20, 5000]), 'max_memory_MB': 27.005952, 'avg_layers_per_token': 'N/A'}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "# -------------------------------\n",
        "# Semantic Scoring Module\n",
        "# -------------------------------\n",
        "class TokenSemanticScorer(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(embed_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        scores = self.fc(x).squeeze(-1)\n",
        "        return torch.sigmoid(scores)  # Between 0 and 1\n",
        "\n",
        "# -------------------------------\n",
        "# Mini Transformer Block\n",
        "# -------------------------------\n",
        "class MiniTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads=2, batch_first=True)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.ln1(x)\n",
        "        attn_output, _ = self.attn(x, x, x)\n",
        "        x = h + attn_output\n",
        "        h = x\n",
        "        x = self.ln2(x)\n",
        "        x = h + self.ff(x)\n",
        "        return x\n",
        "\n",
        "# -------------------------------\n",
        "# Sparse Depth Transformer with Hard Skipping\n",
        "# -------------------------------\n",
        "class SparseDepthTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 512, embed_dim))\n",
        "        self.semantic_scorer = TokenSemanticScorer(embed_dim)\n",
        "        self.layers = nn.ModuleList([MiniTransformerBlock(embed_dim) for _ in range(num_layers)])\n",
        "        self.ln_final = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.size()\n",
        "        x = self.embed(x) + self.pos_embed[:, :T, :]\n",
        "        semantic_scores = self.semantic_scorer(x)  # shape: (B, T)\n",
        "        self.latest_layer_usage = torch.zeros_like(semantic_scores)\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            threshold = i / len(self.layers)\n",
        "            keep_mask = (semantic_scores > threshold)  # shape: (B, T)\n",
        "            self.latest_layer_usage += keep_mask.float()\n",
        "\n",
        "            # Hard skipping: only process tokens above threshold\n",
        "            if keep_mask.any():\n",
        "                mask_expanded = keep_mask.unsqueeze(-1).expand_as(x)\n",
        "                x_new = layer(x.clone())  # clone to prevent in-place errors\n",
        "                x = torch.where(mask_expanded, x_new, x)\n",
        "\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "# -------------------------------\n",
        "# Baseline Transformer (No Sparsity)\n",
        "# -------------------------------\n",
        "class BaselineTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 512, embed_dim))\n",
        "        self.layers = nn.ModuleList([MiniTransformerBlock(embed_dim) for _ in range(num_layers)])\n",
        "        self.ln_final = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.size()\n",
        "        x = self.embed(x) + self.pos_embed[:, :T, :]\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "# -------------------------------\n",
        "# Benchmark Function\n",
        "# -------------------------------\n",
        "def benchmark_model(model, tokens, use_cuda=False):\n",
        "    if use_cuda:\n",
        "        model = model.cuda()\n",
        "        tokens = tokens.cuda()\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    else:\n",
        "        torch.set_num_threads(1)\n",
        "\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        output = model(tokens)\n",
        "    end = time.time()\n",
        "\n",
        "    mem = torch.cuda.max_memory_allocated() / 1e6 if use_cuda else None\n",
        "    layer_usage = getattr(model, \"latest_layer_usage\", None)\n",
        "    return {\n",
        "        \"time_sec\": end - start,\n",
        "        \"output_shape\": output.shape,\n",
        "        \"max_memory_MB\": mem,\n",
        "        \"avg_layers_per_token\": layer_usage.mean().item() if layer_usage is not None else \"N/A\"\n",
        "    }\n",
        "\n",
        "# -------------------------------\n",
        "# Main Test Script\n",
        "# -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    vocab_size = 5000\n",
        "    embed_dim = 64\n",
        "    num_layers = 6\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "\n",
        "    tokens = torch.randint(0, vocab_size, (2, 20))  # batch of 2, 20 tokens\n",
        "\n",
        "    print(\"=== Sparse Depth Transformer ===\")\n",
        "    sparse_model = SparseDepthTransformer(vocab_size, embed_dim, num_layers)\n",
        "    sparse_stats = benchmark_model(sparse_model, tokens, use_cuda)\n",
        "    print(sparse_stats)\n",
        "\n",
        "    print(\"\\n=== Baseline Transformer ===\")\n",
        "    baseline_model = BaselineTransformer(vocab_size, embed_dim, num_layers)\n",
        "    baseline_stats = benchmark_model(baseline_model, tokens, use_cuda)\n",
        "    print(baseline_stats)\n"
      ]
    }
  ]
}